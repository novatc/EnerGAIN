# EnerGAIN

Entwicklung einer intelligenten Gebotsstrategie für den Strommarkt mittels Reinforcement Learning: Eine Untersuchung am
Day-Ahead-Markt und Primärregelleistungsmarkt

## Installation

```bash
pip install -r requirements.txt

pip install stable-baselines3==2.0.0a13
```

## PPO Training Metrics Explanation
## PPO Training Metrics Explanation

When training a PPO model and tracking the progress using TensorBoard, you will come across several important metrics. Here's a breakdown of each metric and its significance:

- **Approx KL**: Approximate Kullback-Leibler (KL) divergence measures the difference between the updated policy and the previous policy. Ideally, this value should remain relatively low and stable throughout training.

- **Clip Fraction**: This value represents the proportion of policy update steps that are clipped during PPO training. A moderate clip fraction between 0.1 and 0.3 is desirable.

- **Clip Range**: Clip range determines the extent of the policy update clipping. It should be set based on the scale of the rewards and should not be too small or too large.

- **Entropy Loss**: Entropy loss encourages exploration by maximizing the entropy of the policy distribution. The ideal entropy loss value is problem-specific, balancing exploration and exploitation.

- **Explained Variance**: This value measures how well the value function estimates match the actual returns observed during training. The closer it is to 1, the better the value function estimation.

- **Policy Gradient Loss**: This value represents the loss incurred by the policy gradient update step. Lower values indicate better policy updates.

- **Standard Deviation (Std)**: This value corresponds to the standard deviation of the action distribution generated by the policy. Its range depends on the specific environment and task requirements.

- **Value Loss**: Value loss represents the error in the value function estimation. Lower values indicate better value function estimates.

